<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <!-- <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1"> -->


  <title>MoMaGen</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" -->
  <!-- rel="stylesheet"> -->

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MoMaGen:</h1>
            <h2 class="subtitle is-2 publication-subtitle">Generating Demonstrations under Soft and Hard Constraints
for Multi-Step Bimanual Mobile Manipulationn</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://arpitrf.github.io/" target="_blank">Arpit Bahety</a>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/arnav-balaji-402ba2280/" target="_blank">Arnav Balaji</a>,</span>
                  <span class="author-block">
                    <a href="https://babbatem.github.io/" target="_blank">Ben Abbatematteo</a>,</span>
                  <span class="author-block">
                    <a href="https://robertomartinmartin.com/" target="_blank">Roberto Martín-Martín</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The University of Texas at Austin
                      <!-- <br>Conferance name and year -->
                    </span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <br>
                  <!-- <span class="is-size-4 publication-venue">Robotics: Science and Systems (RSS), 2025<span> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Paper PDF link -->
                      <span class="link-block">
                        <a href="https://www.roboticsproceedings.org/rss21/p128.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- ArXiv abstract Link -->
                  <!-- <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                  </span>

                  <!-- Poster Link -->
                  <span class="link-block">
                    <a href="https://drive.google.com/file/d/1wJD5NEzMDj5kSYW376Lvxqul4sfVfSKr/view?usp=drive_link" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-file"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>

                <!-- YouTube Link -->
                <span class="link-block">
                  <a href="https://youtu.be/p7WWkZ-vO0c" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-play-circle"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/teaser_video.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            For robots to become efficient helpers in the home, they must learn to perform new mobile manipulation tasks simply by watching humans perform them. Learning from a single video demonstration from a human is challenging as the robot needs to first extract from the demo what needs to be done and how, translate the strategy from a third to a first-person perspective, and then adapt it to be successful with its own morphology. Furthermore, to mitigate the dependency on costly human monitoring, this learning process should be performed in a safe and autonomous manner. We present SafeMimic, a framework to learn new mobile manipulation skills safely and autonomously from a single third-person human video. Given an initial human video demonstration of a multi-step mobile manipulation task, SafeMimic first parses the video into segments, inferring both the semantic changes caused and the motions the human executed to achieve them and translating them to an egocentric reference.  Then, it adapts the behavior to the robot's own morphology by sampling candidate actions around the human ones, and verifying them for safety before execution in a receding horizon fashion using an ensemble of safety Q-functions trained in simulation. When safe forward progression is not possible, SafeMimic backtracks to previous states and attempts a different sequence of actions, adapting both the trajectory and the grasping modes when required for its morphology.  As a result, SafeMimic yields a strategy that succeeds in the demonstrated behavior and learns task-specific actions that reduce exploration in future attempts. Our experiments show that our method allows robots to safely and efficiently learn multi-step mobile manipulation behaviors from a single human demonstration, from different users, and in different environments, with improvements over state-of-the-art baselines across seven tasks.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="text-align: center;">Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/p7WWkZ-vO0c?si=wNNN4OPzONX5SBUu" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<section class="section">
  <div class="container">
    <h2 class="title is-2" style="text-align: center;">Overview</h2>
    <div class="columns is-centered">
      <div class="column is-four-fifths is-centered has-text-centered">
        <div class="content has-text-justified">
          <p>
            <!-- From an RGB-D video of a human performing a multi-step mobile manipulation task acquired by the robot, SafeMimic uses a combination of human pose tracking models
            and VLM prompting to perform coarse-to-fine segmentation obtaining semantic changes, "what?" and human action trajectories, "how?", 
            and translating them to the robot's point of view (left). 
            SafeMimic then refines and adapts each task segment safely by sampling and verifying actions before executing them thanks to an ensemble of safety Q-functions pretrained in simulation, 
            Q<sub>safe</sub> (top right). 
            If forward progress is not possible, SafeMimic autonomously backtracks and tries different actions (orange arrows). 
            When required to overcome large differences in morphology, SafeMimic explores alternative grasp modes (second row of samples), adapting the grasp to enable successful execution. 
            Successful attempts are detected by a VLM that verifies when the parsed semantic change of the segment is achieved. Successes are stored and used to train a policy memory module with geometric augmentations  (bottom right) 
            that predicts actions (grasps or action trajectories) in subsequent attempts, given a pointcloud and language task description, in order to reduce the need for exploration. -->
            SafeMimic consists of three steps. The first step extracts the semantic segments (what the human did) and the human actions (how did the human do it) from a single third-person video.
            The next step helps the robot to safely and autonomously adapt the human actions to the robot's own embodiment. Finally, in the third step the robot uses the policy memory module to 
            learn from the successful trajectories to reduce the exploration in future trials.
          </p>
        </div>
        <!-- <img src="static/images/SafeMimic_Method.png" alt="SafeMimic Overview" width="100%"/> -->
        <video poster="" id="video3" autoplay controls muted loop height="100%" style="margin: 1.0%;">
          <!-- Your video file here -->
          <source src="static/videos/safemimic_website_method.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container">
    <h2 class="title is-2" style="text-align: center;">Learning Safety Q-Functions</h2>

    <div class="columns is-centered">
      <div class="column is-four-fifths is-centered has-text-centered">
        <div class="content has-text-justified">
          <p>
          Training Safety Q-functions in the real world would
          be dangerous, as the robot needs to experience and learn what
          actions may lead to unsafe states and when. Therefore, we
          pretrain an ensemble of Safety Q-functions in simulation, one
          for each type of unsafe transition. 
          The safety Q-value is 1 if the action (a) from the state (s) is unsafe else it's 0 (left video).
          The ensemble of Safety Q-functions is pretrained in domain-randomized environments in
          the simulator OmniGibson in different scenarios, including 
          articulated object interaction, rigid-body pick-and-place,
          and base navigation by sampling random and noise-corrupted
          task-related actions (right video). 
        </p>
        </div>
      </div>
    </div>
  
    <!-- <div class="columns is-centered">
      <div class="column is-two-fifths">
        <video poster="" id="video2" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
          <source src="static/videos/sim_1.mov"
          type="video/mp4">
        </video>
      </div>
      <div class="column is-two-fifths">
        <video poster="" id="video3" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
          <source src="static/videos/sim_mosaic.mp4"
          type="video/mp4">
        </video>
      </div>
    </div> -->

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="columns is-vcentered">
          <!-- Left video: narrower -->
          <div class="column" style="flex: 0 0 30%; max-width: 30%;">
            <video id="video2" autoplay controls muted loop style="width: 100%; height: auto; margin: 1.0%;">
              <source src="static/videos/sim_1.mov" type="video/mp4">
            </video>
          </div>

          <!-- Right video: wider -->
          <div class="column" style="flex: 0 0 70%; max-width: 70%;">
            <video id="video3" autoplay controls muted loop style="width: 100%; height: auto; margin: 1.0%;">
              <source src="static/videos/sim_mosaic.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
<div class="container">
  <h2 class="title is-2" style="text-align: center;">Experiments</h2>
  
  <div class="columns is-centered">
    <div class="column is-two-fifths">
      <h3 style="text-align: center; font-weight: bold;">Human Demonstration</h3>
      <video poster="" id="video2" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/human_fill_pot.mov"
        type="video/mp4">
      </video>
    </div>
    <div class="column is-two-fifths">
      <h3 style="text-align: center; font-weight: bold;">Robot Execution</h3>
      <video poster="" id="video3" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/robot_fill_pot.mov"
        type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-centered" style="background-color: #f5f5f5;">
    <div class="column is-two-fifths">
      <video poster="" id="video2" autoplay controls muted height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/human_store_in_drawer.mov"
        type="video/mp4">
      </video>
    </div>
    <div class="column is-two-fifths">
      <video poster="" id="video3" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/robot_store_in_drawer.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-centered">
    <div class="column is-two-fifths">
      <video poster="" id="video2" autoplay controls muted height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/human_erase_whiteboard.mov"
        type="video/mp4">
      </video>
    </div>
    <div class="column is-two-fifths">
      <video poster="" id="video3" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/robot_erase_whiteboard.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-centered" style="background-color: #f5f5f5;">
    <div class="column is-two-fifths">
      <video poster="" id="video2" autoplay controls muted height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/human_load_oven.mov"
        type="video/mp4">
      </video>
    </div>
    <div class="column is-two-fifths">
      <video poster="" id="video3" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/robot_load_oven.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-centered">
    <div class="column is-two-fifths">
      <video poster="" id="video2" autoplay controls muted height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/human_shelving_item.mov"
        type="video/mp4">
      </video>
    </div>
    <div class="column is-two-fifths">
      <video poster="" id="video3" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/robot_shelving_item.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-centered" style="background-color: #f5f5f5;">
    <div class="column is-two-fifths">
      <video poster="" id="video2" autoplay controls muted height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/human_box_item.mov"
        type="video/mp4">
      </video>
    </div>
    <div class="column is-two-fifths">
      <video poster="" id="video3" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/robot_box_item.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-centered">
    <div class="column is-two-fifths">
      <video poster="" id="video2" autoplay controls muted height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/human_refrigerate_item.mov"
        type="video/mp4">
      </video>
    </div>
    <div class="column is-two-fifths">
      <video poster="" id="video3" autoplay controls muted loop height="100%" style="border: 1px solid #bbb; border-radius: 10px; margin: 1.0%;">
        <!-- Your video file here -->
        <source src="static/videos/robot_refrigerate_item.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-centered">
    <div class="column is-two-fifths is-offset-two-fifths">
      <p class="has-text-right has-text-grey" style="font-size: 0.9rem;">
        All robot videos at 40×
      </p>
    </div>
  </div>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
